[["index.html", "NEON Ecological Forecasting Challenge Preface", " NEON Ecological Forecasting Challenge Ecological Forecasting Initative Research Coordination Network 2021-04-01 Preface The NSF funded EFI Research Coordination Network (EFI-RCN) is hosting a NEON Ecological Forecast Challenge with the goal to create a community of practice that builds capacity for ecological forecasting by leveraging NEON data products. The Challenge revolves around the five theme areas listed below that span aquatic and terrestrial systems, and population, community, and ecosystem processes across a broad range of ecoregions that uses data collected by NEON. As a community, we are excited to learn more about the predictability of ecological processes by forecasting NEON data prior to its release. What modeling frameworks, mechanistic processes, and statistical approaches best capture community, population, and ecosystem dynamics? These questions are answerable by a community generating a diverse array of forecasts. The Challenge is open to any individual or team that wants to submit forecasts and includes categories for different career stages. Individuals or team contacts can register to submit forecasts HERE. The design of the Challenge is the result of contributions of over 200 participants in the May 2020 virtual EFI-RCN meeting, including partner organizations, and the hard work from the Design Teams that have developed the protocols for each of the themes. Computational resources are supported by NSF funded CyVerse, Jetstream, and XSEDE. Here are videos from the December 9, 2020 AGU EFI Town Hall providing an overview of 1) of the Challenge, 2) the Challenge cyberinfrastructure, and 3) the NEON data streams. "],["theme-aquatic-ecosystems.html", "Chapter 1 Theme: Aquatic Ecosystems 1.1 Overview 1.2 Challenge 1.3 Data: Training and Evaluation 1.4 Data: Targets 1.5 Detailed Protocol 1.6 Timeline 1.7 Design Team 1.8 Partners 1.9 Details 1.10 References", " Chapter 1 Theme: Aquatic Ecosystems WHAT: Freshwater temperature and dissolved oxygen WHERE: 1 lake and 1 river NEON sites. Click to expand the image in a new tab. WHEN: Daily forecasts with a 7-day forecast horizon at the beginning of the month and submitted monthly from May 31-August 2021; later submissions after the May 31 start are permissible WHY: Temperature and oxygen are critical for life in aquatic environments and can represent the health of the system WHO: Open to any individual or team that registers HOW: REGISTER your team and submit forecast 1.1 Overview In streams and rivers, forecasting water temperature can be meaningful for protecting aquatic communities while maintaining socio-economic benefits (Ouellet-Proulx et al. 2017). In lentic systems, successfully forecasting surface water temperatures can be important for fisheries and water utilities that need to manage the outflowing temperatures (Zhu et al. 2020). Recently, water temperature forecasts in lakes have been used to predict seasonal turnover when nutrients from the bottom can be mixed to the surface and impair the water quality. Dissolved oxygen concentration is a critically important variable in limnology. Forecasts of dissolved oxygen in freshwaters is the first step to understanding other freshwater ecosystem processes. For example, oxygen serves as the gatekeeper to other biogeochemical reactions that occur in rivers and lakes. Preemptive forecasts of dissolved oxygen concentrations can anticipate periods of high or low oxygen availability, thereby providing insight into how the ecosystem may change at relatively short timescales. 1.2 Challenge This design challenge asks teams to produce forecasts of mean daily surface water temperature and/or dissolved oxygen in one NEON lake and/or one NEON river site in the Southeastern U.S. 35 days from the first of the month. The NEON lake site is Barco Lake (BARC) in Florida and the NEON river site is Posey Creek (POSE) in Virginia. Each forecast will start on the 1st day of each month and must forecast up to 7 days into the future. Forecasts are welcome to go past the 7 day timeline but those dates will not be evaluated. Teams are asked to submit their 7 day forecasts of NEON surface water temperature and/or dissolved oxygen measurements along with uncertainty estimates and metadata. Any NEON surface water temperature and/or dissolved oxygen data prior to the 7 days being forecasted will be provided and may be used to build and improve the forecast models. Other data (other than temperature and/or dissolved oxygen data provided from NEON) can be used so long as they are not from the 7 days being forecasted at the beginning of each month, that they are publicly available, and that teams provide access (minimum of URL, but ideally a script) to all teams in the challenge. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation (PENDING). Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). 1.3 Data: Training and Evaluation The R script for generating the evaluation and training data can be found at: https://github.com/eco4cast/neon4cast-aquatics The challenge uses the following NEON data products: DP1.20264.001: Temperature at specific depth in surface water DP1.20288.001: Water quality 1.4 Data: Targets A file with previously released NEON data that has been processed into “targets” is provided below. The target script can be found here. The same processing will be applied to new data that are used for forecast evaluation. Before the Aquatics challenge begins, a processing script will be available in the neon4cast-aquatics GitHub repository. 1.5 Detailed Protocol Details of the targets, how they are calculated, descriptions of the target files, and examples of other environmental variables that could be used in the Challenge are HERE. Access Targets Download an example of a forecast output format for submission HERE 1.6 Timeline The timeline is determined by the data latency provided by NEON. NEON data is released in month long sets, 2 weeks after the month ends. NEON data for a given month is scheduled to be released around the 15th of the following month. Once the NEON data for a previous month is released, teams have between the release of those data to the end of the month to forecast the 7 days of the current month (see table). Forecast submissions will due beginning May 31, 2021 at 11:59 Eastern Standard Time (UTC−05:00) for forecasts that start May 1. Final forecast submissions will be due on August 31, 2021 at 11:59 Eastern Standard Time (UTC−05:00) for forecasts that start August 1. As an example, if NEON water temperature data is released on April 15 for data from March 1 - 31, teams then can use these new March data and the NOAA GEFS forecast issued on April 1 at 00:00 to help generate forecasts from April 1 - April 8 (7 days). This April forecast is due by 11:59 pm EST on April 30. The forecast issue date for the April forecast is April 1, so no new observational data from after that date can be used to constrain forecasts and the forecast should use the weather forecast issued at midnight April 1 (i.e. start of day) as the driver (not the observed meteorology in April or forecasts made at later dates). Evaluation will occur as new NEON data is released. Forecast Timeline Table 1.7 Design Team James Guinnip, Kansas State University Sarah Burnet, University of Idaho Ryan McClure, Virginia Tech Chris Brown, National Oceanic and Atmospheric Administration Cayelan Carey, Virginia Tech Whitney Woelmer, Virginia Tech Jake Zwart, United States Geological Survey 1.8 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge are from the National Ecological Observatory Network (NEON): https://www.neonscience.org/. Scientists from NOAA and USGS have been involved in the design of the challenge. 1.9 Details 1.9.1 Focal variables 1.9.1.1 Surface Mean Daily Dissolved Oxygen Concentration Definition Dissolved oxygen (DO) is the concentration of oxygen dissolved in water. NEON’s 30-minute time resolution from deployed water quality sondes among the freshwater sites reports this concentration as mg L-1. We have adapted the available NEON DO data to output the mean daily DO concentration in mg L-1 from a water quality sonde deployed 1m below the water surface at a lake site (Barco Lake) and a water quality sonde deployed in a stream site (Posey Creek). Common DO concentrations range between 0 and 12 mg L-1 and DO concentrations less than 2 mg L-1 are considered hypoxic. Motivation Dissolved oxygen concentration is a critically important variable in limnology. Forecasts of dissolved oxygen in freshwaters is the first step to understanding other freshwater ecosystem processes. For example, oxygen serves as the gatekeeper to other biogeochemical reactions that occur as well as determine the variety and health of aquatic organisms present in rivers and lakes. Preemptive forecasts of dissolved oxygen concentrations can anticipate periods of high or low oxygen availability, thereby providing insight into how the ecosystem may change at relatively short timescales. 1.9.1.2 Surface Mean Daily Water Temperature Definition Water temperature is the temperature of the water. NEON’s 30-minute time resolution from deployed water temperature sondes in the freshwater sites reports this in degrees celsius (°C). We have adapted the available NEON water temperature data to output the mean daily water temperature in °C from a water temperature sonde deployed 1m below the water surface at a lake site (Barco Lake) and a water temperature sonde deployed in a stream site (Posey Creek). Common water temperatures in lakes and streams range between 4 and 35 °C. Motivation In streams and rivers, forecasting water temperature can be meaningful for protecting aquatic communities while maintaining socio-economic benefits (Ouellet-Proulx et al. 2017). In lentic and lotic systems, successfully forecasting water temperatures can be important for management of fisheries and water utilities that rely on specific threshold temperatures (Zhu et al. 2020). Recently, lake temperature forecasts have been used to predict seasonal turnover, mixing bottom nutrients into the surface and impairing water quality.Data 1.9.2 Focal sites The challenge is focused on two freshwater NEON sites including a lake and stream ecosystem. Table 1 lists the sites. 1.10 References Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 Ouellet-Proulx, S., St-Hilaire, A., and Bouchar, M.-A.. 2017. Water temperature ensemble forecasts: Implementation using the CEQUEAU model on two contrasted river systems. Water 9(7):457. https://doi.org/10.3390/w9070457 Zhu, S., Ptak, M., Yaseen, Z.M., Dai, J. and Sivakumar, B. 2020. Forecasting surface water temperature in lakes: a comparison of approaches. Journal of Hydrology 585, 124809. https://doi.org/10.1016/j.jhydrol.2020.124809 "],["participation.html", "Chapter 2 Participation 2.1 Participation guidance 2.2 Submission instructions 2.3 Participation agreement 2.4 NEON Data Use 2.5 Additional data options", " Chapter 2 Participation 2.1 Participation guidance 2.1.1 HOW TO PARTICIPATE: Participation requires that teams: 1) Complete a REGISTRATION for each forecast theme you are participating in and each model you are contributing within a theme 2) Agree to the participation agreement below 3) Submit forecast netCDF or csv file(s) 4) Provide the metadata xml file documenting the forecast One contact person should register on behalf of their team. That contact person will be asked to provide the group members’ names, emails, and affiliations so that everyone in the group can receive an invitation to join the Challenge theme Slack channel and access group resources. Teams are allowed and encouraged to join the challenge after the start date of each Challenge theme because there are multiple deadlines to submit forecasts. However, only forecasts submitted by each submission deadline will be officially scored. 2.1.2 TEAMS: Teams can be individuals or groups. They can represent institutions or organizations. You will have 25 characters for a team name (e.g., “EFI Null Model”) and 10 characters for the team name ID (no spaces allowed; e.g., “EFI_Null”). The registration includes team categories (e.g., undergraduate only, graduate only, multi-institution, etc). Please check all that apply. If your team wants to submit multiple forecasts, please register a team for each model as only one forecast model per cycle per team is allowed. If there are two different time steps in a challenge theme (e.g., the terrestrial carbon flux theme has a 30-minute and 1-day option), register each as separate teams. 2.1.3 SLACK AND GITHUB COMMUNICATION We strongly encourage participants to use the Challenge theme Slack channels to ask questions, discuss ideas and challenges, and share resources. Overall, we strongly encourage a collegial approach to the Challenge – this is a friendly competition to move the field forward and bring more people into the community, not a cutthroat competition to win by denying other teams useful information. GitHub repositories for each Challenge theme will be available with helper code and an example workflow (null models). We encourage teams to contribute code to these repositories (via Pull Request) if they develop additional helper code. This is especially important if an individual or group is going to add additional data constraints to their forecast. Remember, the use of data external to NEON is allowed and encouraged so long as it is publicly available and other teams are notified about it. Also, while most anything could be used to calibrate parameters and constrain initial conditions, only other forecasts (e.g. weather) can be used as drivers/covariates during the actual forecast period. 2.2 Submission instructions 2.3 Participation agreement All participants agree to have their forecast posted in real-time on the NEON Ecological Forecast Challenge Output RShiny app (in development) and potentially published in a scientific journal. The manuscripts describing the accuracy of forecasts across teams will be coordinated by the Ecological Forecasting Initiative Research Coordination Network and extend authorship to members of each team with an opt-in policy. If a publication is generated by a forecast team, we ask that the manuscript acknowledge the Ecological Forecasting Initiative Research Coordination Network and its support from the National Science Foundation (DEB-1926388). 2.4 NEON Data Use NEON data products, software, and derivatives thereof are freely available for use when accompanied by appropriate disclaimers, acknowledgments, and data citations, defined in the NEON data use policy. 2.5 Additional data options Individuals and groups may create forecasts that use other publicly available data in addition to the NEON data, so long as other teams participating in the challenge are notified about the existence of the data via the Challenge theme’s Slack channel. Teams are encouraged to make available the code they are using to access, download, and process any additional data constraints they are using, ideally via a pull request to each Challenge Github repo. When considering the use of data in forecasts it is important to distinguish data that are being used as drivers/covariates during each forecast from data being used to constrain model structure, parameters, initial conditions, and error distributions. While the latency of NEON data requires that some of our forecast will be (fully or partly) hindcasts, all forecasts should be run as if they are true forecasts – you cannot use any observed data as a driver/covariate or constraint during the forecast period itself as that info would not have been available at the forecast start date. For example, if you find that a particular variable is a useful covariate during the model development &amp; calibration period (e.g. soil temperature) then you would need to find or make a forecast of that variable if you want to use it as a covariate. Teams using meteorological covariates should use the shared meteorological driver data provided by EFI (see Shared Forecast Drivers). As an example of potentially useful external data, each NEON site has subsets of various remote sensing products that are hosted on the ORNL DAAC (ORNL DAAC subsets). These include: MODIS collection 6: LAI, FPAR, burned area, surface reflectance, land surface temperature, vegetation indices (NDVI, EVI), modeled ET, GPP, NPP. VIIRS collection 1: surface reflectance, vegetation indices, LAI, FPAR, land surface temperature, SMAP: modeled NEE, GPP, Rh, SOC Daymet: daily surface weather data "],["shared-forecast-drivers.html", "Chapter 3 Shared Forecast Drivers 3.1 Meteorology: NOAA Global Ensemble Forecasting System 3.2 Meteorology: NEON Observed", " Chapter 3 Shared Forecast Drivers We are downloading, subsetting, and processing forecasted meteorology drivers for each NEON site. Currently, we have NOAA’s Global Ensemble Forecasting System V12 output available at the 1 hr time resolution for each NEON site. For forecasts generated at midnight (00) UTC, the forecasts extend 35-days in the future. For forecasts generated at 06, 12, and 18 UTC, the forecasts extend 16-days in the future. There are 31 ensemble members for each forecast. Each ensemble member is available as a separate netcdf file. The following meteorological variables are included: air temperature, air pressure, wind speed, precipitation, downwelling longwave radiation, downwelling shortwave radiation, relative humidity, specific humidity, and total cloud cover. The weather forecasts are available through an s3 bucket (see Meteorology: NOAA Global Ensemble Forecasting System below) with multiple ways to access them: You can click on a file in the browser, you can directly download individual files from the command line using the file address, or you can download multiple files using aws.s3 commands. We provide an example using the aws.s3 package in R for downloading all the ensemble members for particular location, forecast cycle (00, 06, 12, or 18), and NEON site at: https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/download_noaa_files_s3.R Additionally we provide example code in R for converting the netcdf files to csv files https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/noaa_gefs_read_example.R https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/noaa_gefs_read.R 3.1 Meteorology: NOAA Global Ensemble Forecasting System 1 Hour NOAA Drivers 3.2 Meteorology: NEON Observed In development through collaboration with NEON and NCAR. "],["evaluation.html", "Chapter 4 Evaluation 4.1 Results 4.2 Scoring Metric: Continuous Ranked Probability Score 4.3 Null forecast 4.4 Forecast Submission Visualization and Leaderboard", " Chapter 4 Evaluation Forecasts will be evaluated at each site and forecast horizon (i.e., time-step into the future), and a summary score will be assigned evaluating overall performance of all forecast submissions across sites. Forecasts will also be compared to a null model. Forecast evaluation results will be presented for all submitted models together and separately for each team category: undergraduate student only team, graduate student only team, post-doc only team, single institution team, multi-institution team, international team (team with individuals from at least two countries). 4.1 Results Preliminary results will be distributed using the NEON Ecological Forecast Challenge Output RShiny app and at https://data.ecoforecast.org/minio/scores/. We intend to write a joint manuscript synthesizing forecasts. Teams are welcome to publish results from their model at any time. If a publication is generated we encourage the manuscript to acknowledge the Ecological Forecasting Research Coordination Network and its support from the National Science Foundation (DEB-1926388). 4.2 Scoring Metric: Continuous Ranked Probability Score Forecasts will be scored using the continuous ranked probability score (CRPS), a proper scoring rule for evaluating forecasts presented as distributions or ensembles (Gneiting &amp; Raftery 2007). The CRPS compares the forecast probability distribution to that of the validation observation and assigns a score based on both the accuracy and precision of the forecast. We will use the ‘crps_sample’ function from the scoringRules package in R to calculate the CRPS for each forecast. We will generate a combined score for all locations and forecast horizons. Forecasts will also be evaluated using the CRPS at each time-step in the forecast horizon and each location included in the forecasts. 4.2.1 Example of a CRPS calculation from an ensemble forecast The following uses Equation 2 in Jordan, Kruger, and Lerch 2018 Equation 1 from Jordan, Kruger, and Lerch 2018. First, create a random sample from a probability distribution. This is the “forecast” for a particular point in time. For simplicity, we will use a normal distribution with a mean of 8 and standard deviation of 1 x &lt;- rnorm(1000, mean = 8, sd = 1.0) Second, we have our data point (i.e., the target). We will set it to zero as well y &lt;- 8 Now calculate CRPS using Equation 2 s &lt;- 0 for(i in 1:length(x)){ for(j in 1:length(x)){ s &lt;- s + abs(x[i] - x[j]) } } crps_equation_2 &lt;- mean(abs(x - y)) - s / (2 * length(x)^2) crps_equation_2 ## [1] 0.2337715 Now calculate using the crps_sample() function in the scoringRules package crps_sample(y = y, dat = x) ## [1] 0.2337715 4.2.2 Exploring the scoring surface Now lets see how the CRPS changes as the mean and standard deviation of the forecasted distribution change First, set vectors for the different mean and SD values we want to explore sample_mean &lt;- seq(4, 12, 0.1) sample_sd &lt;- seq(0.1, 10, 0.1) Second, set our observed value to 8 for simplicity y &lt;- 8 Now calculate the CRPS at each combination of forest mean and SD combined &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd))) for(i in 1:length(sample_mean)){ for(j in 1:length(sample_sd)){ sample &lt;- rnorm(10000, sample_mean[i], sample_sd[j]) combined[i, j] &lt;- crps_sample(y = y, dat = sample) } } Finally, visualize the scoring surface with the observed value represented by the red line contour(x = sample_mean, y = sample_sd, z = as.matrix(combined),nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) The contour surface highlights the trade-off between the mean and standard deviation. 4.2.3 CRPS from the Normal Distribution If the distributional forecast is a normal distribution represented by a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), an ensemble of predictions is not needed to evaluate CRPS because we can take advantage of the analytic solution to CRPS under the normal assumption (Equation 4 from Calibrated Probabilistic Forecasting Using Ensemble Model Output Statistics and Minimum CRPS Estimation). Equation 5 from Calibrated Probabilistic Forecasting Using Ensemble Model Output Statistics and Minimum CRPS Estimation gives \\[\\begin{align*} CRPS(N(\\mu, \\sigma^2) | y) = \\sigma \\left( \\frac{y - \\mu}{\\sigma} \\left( 2 \\Phi\\left( \\frac{y - \\mu}{\\sigma} \\right) - 1 \\right) + 2 \\phi \\left( \\frac{y - \\mu}{\\sigma} \\right) - \\frac{1}{\\sqrt{\\pi}} \\right) \\end{align*}\\] for \\(\\Phi(\\cdot)\\) and \\(\\phi(\\cdot)\\) the standard normal CDF and PDF, respectively. Therefore, if the forecast distribution is truly a normal distribution (often this isn’t true in forecasts that only report a mean and sd) a simplified score can be applied as follows: sample_mean &lt;- seq(4, 12, 0.1) sample_sd &lt;- seq(0.1, 10, 0.1) combined_norm &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd))) for(i in 1:length(sample_mean)){ for(j in 1:length(sample_sd)){ combined_norm[i, j] &lt;- crps_norm(y = y, mean = sample_mean[i], sd = sample_sd[j]) } } Finally, visualize the scoring surface with the observed value represented by the red line contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) Note that at a given value of the sd, the lowest score is achieved at \\(\\mu = y\\) as shown for each of the blue lines where the minmum value of the score across each blue line is at the red line. This behavior make sense because the CRPS is a score that reward accuracy and precision. Thus, for any given level of precision (represented by the standard deviation), CRPS is optimized by producing the most accurate prediction of the distribution’s location. contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) abline(h = 2.5, col = &quot;blue&quot;) abline(h = 4.3, col = &quot;blue&quot;) abline(h = 6.8, col = &quot;blue&quot;) Interestingly, for a given mean \\(\\mu \\neq y\\) we find a pattern that makes intuitive sense given the goal of CRPS to produce forecasts that are both accurate and precise. For a given amount of bias in the prediction (i.e., given a \\(\\mu \\neq y\\)), the optimal score is achieved by a standard deviation that slightly larger than the bias layout(matrix(1:4, 2, 2, byrow = TRUE)) ## plots for mu = 7 mu &lt;- 7 contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;, main = paste0(&quot;CRPS contour given mu = &quot;, mu)) abline(v = mu, col = &quot;red&quot;) min_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))] abline(h = min_sd, col = &quot;blue&quot;) plot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = &#39;l&#39;, main = paste0(&quot;CRPS profile given mu = &quot;, mu)) abline(v = min_sd, col = &quot;blue&quot;) ## plots for mu = 11 mu &lt;- 11 contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;, main = paste0(&quot;CRPS contour given mu = &quot;, mu)) abline(v = mu, col = &quot;red&quot;) min_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))] abline(h = min_sd, col = &quot;blue&quot;) plot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = &#39;l&#39;, main = paste0(&quot;CRPS profile given mu = &quot;, mu)) abline(v = min_sd, col = &quot;blue&quot;) Next, we plot the relationship between a given value of \\(\\mu\\) and the \\(\\sigma\\) that produces the optimal CRPS. This looks like a linear relationship. optimal_sd &lt;- rep(0, length(sample_mean)) for (i in 1:length(sample_mean)) { optimal_sd[i] &lt;- sample_sd[which.min(crps_norm(y, mean = sample_mean[i], sd = sample_sd))] } plot(sample_mean, optimal_sd, type = &#39;l&#39;) Let’s estimate the slope of the relationship. It looks like the optimal \\(sd\\) for a normal distribution forecast that is biased by \\(|y - \\mu|\\) is \\(sd = 1.2|y - \\mu|\\) which makes sense as this would put the true value in a region of high probability. coef(lm(optimal_sd[sample_mean &gt; 0] ~ sample_mean[sample_mean &gt; 0])) ## (Intercept) sample_mean[sample_mean &gt; 0] ## 2.430864e+00 -1.688326e-16 4.3 Null forecast All forecasts will be compared to a null forecast produced by a simple historical-means calculation or a random walk. The GitHub repository for each theme has the code for the null model. 4.4 Forecast Submission Visualization and Leaderboard The dashboard shows the forecast submissions by each team for each forecast theme by date and forecast variable. It also provides the CRPS scores for each submitted forecast. "]]
