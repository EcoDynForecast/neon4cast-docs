[["index.html", "NEON Ecological Forecasting Challenge Introduction", " NEON Ecological Forecasting Challenge Ecological Forecasting Initative Research Coordination Network 2021-04-02 Introduction The NSF funded EFI Research Coordination Network (EFI-RCN) is hosting a NEON Ecological Forecast Challenge with the goal to create a community of practice that builds capacity for ecological forecasting by leveraging NEON data products. The Challenge revolves around the five theme areas listed below that span aquatic and terrestrial systems, and population, community, and ecosystem processes across a broad range of ecoregions that uses data collected by NEON. As a community, we are excited to learn more about the predictability of ecological processes by forecasting NEON data prior to its release. What modeling frameworks, mechanistic processes, and statistical approaches best capture community, population, and ecosystem dynamics? These questions are answerable by a community generating a diverse array of forecasts. The Challenge is open to any individual or team that wants to submit forecasts and includes categories for different career stages. Individuals or team contacts can register to submit forecasts HERE. The design of the Challenge is the result of contributions of over 200 participants in the May 2020 virtual EFI-RCN meeting, including partner organizations, and the hard work from the Design Teams that have developed the protocols for each of the themes. Computational resources are supported by NSF funded CyVerse, Jetstream, and XSEDE. Here are videos from the December 9, 2020 AGU EFI Town Hall providing an overview of 1) of the Challenge, 2) the Challenge cyberinfrastructure, and 3) the NEON data streams. "],["theme-aquatic-ecosystems.html", "1 Theme: Aquatic Ecosystems 1.1 Overview 1.2 Challenge 1.3 Data: Training and Evaluation 1.4 Data: Targets 1.5 Timeline 1.6 Design Team 1.7 Partners 1.8 Details 1.9 References", " 1 Theme: Aquatic Ecosystems What: Freshwater temperature and dissolved oxygen Where: 1 lake and 1 river NEON sites. Click to expand the image in a new tab. When: Daily forecasts with a 7-day forecast horizon at the beginning of the month and submitted monthly from May 31-August 2021; later submissions after the May 31 start are permissible Why: Temperature and oxygen are critical for life in aquatic environments and can represent the health of the system Who: Open to any individual or team that registers How: REGISTER your team and submit forecast 1.1 Overview In streams and rivers, forecasting water temperature can be meaningful for protecting aquatic communities while maintaining socio-economic benefits (Ouellet-Proulx et al. 2017). In lentic systems, successfully forecasting surface water temperatures can be important for fisheries and water utilities that need to manage the outflowing temperatures (Zhu et al. 2020). Recently, water temperature forecasts in lakes have been used to predict seasonal turnover when nutrients from the bottom can be mixed to the surface and impair the water quality. Dissolved oxygen concentration is a critically important variable in limnology. Forecasts of dissolved oxygen in freshwaters is the first step to understanding other freshwater ecosystem processes. For example, oxygen serves as the gatekeeper to other biogeochemical reactions that occur in rivers and lakes. Preemptive forecasts of dissolved oxygen concentrations can anticipate periods of high or low oxygen availability, thereby providing insight into how the ecosystem may change at relatively short timescales. 1.2 Challenge This design challenge asks teams to produce forecasts of mean daily surface water temperature and/or dissolved oxygen in one NEON lake and/or one NEON river site in the Southeastern U.S. 35 days from the first of the month. The NEON lake site is Barco Lake (BARC) in Florida and the NEON river site is Posey Creek (POSE) in Virginia. Each forecast will start on the 1st day of each month and must forecast up to 7 days into the future. Forecasts are welcome to go past the 7 day timeline but those dates will not be evaluated. Teams are asked to submit their 7 day forecasts of NEON surface water temperature and/or dissolved oxygen measurements along with uncertainty estimates and metadata. Any NEON surface water temperature and/or dissolved oxygen data prior to the 7 days being forecasted will be provided and may be used to build and improve the forecast models. Other data (other than temperature and/or dissolved oxygen data provided from NEON) can be used so long as they are not from the 7 days being forecasted at the beginning of each month, that they are publicly available, and that teams provide access (minimum of URL, but ideally a script) to all teams in the challenge. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). 1.3 Data: Training and Evaluation The R script for generating the evaluation and training data can be found at: https://github.com/eco4cast/neon4cast-aquatics The challenge uses the following NEON data products: DP1.20264.001: Temperature at specific depth in surface water DP1.20288.001: Water quality 1.4 Data: Targets A file with previously released NEON data that has been processed into “targets” is provided below. The target script can be found here. The same processing will be applied to new data that are used for forecast evaluation. Before the Aquatics challenge begins, a processing script is available in the neon4cast-aquatics GitHub repository. Here is the format of the target file readr::read_csv(&quot;https://data.ecoforecast.org/targets/aquatics/aquatics-targets.csv.gz&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## time = col_date(format = &quot;&quot;), ## siteID = col_character(), ## oxygen = col_double(), ## temperature = col_double(), ## oxygen_sd = col_double(), ## temperature_sd = col_double(), ## depth_oxygen = col_double(), ## depth_temperature = col_double(), ## neon_product_ids = col_character() ## ) ## # A tibble: 2,556 x 9 ## time siteID oxygen temperature oxygen_sd temperature_sd depth_oxygen ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2017-10-20 BARC 6.01 26.4 0.0114 0.00104 1.19 ## 2 2017-10-21 BARC 6.06 26.3 0.00717 0.000986 1.20 ## 3 2017-10-22 BARC 6.13 26.4 0.00721 0.00119 1.19 ## 4 2017-10-23 BARC 6.43 26.6 0.00765 0.000944 1.16 ## 5 2017-10-24 BARC 6.31 26.4 0.00746 0.00118 1.11 ## 6 2017-10-25 BARC 6.27 25.9 0.00737 0.00107 1.13 ## 7 2017-10-26 BARC 6.30 25.0 0.00743 0.00105 1.19 ## 8 2017-10-27 BARC 6.41 24.3 0.00762 0.00104 1.22 ## 9 2017-10-28 BARC 6.48 24.1 0.00766 0.00111 1.17 ## 10 2017-10-29 BARC 6.64 23.8 0.00783 0.00101 1.10 ## # … with 2,546 more rows, and 2 more variables: depth_temperature &lt;dbl&gt;, ## # neon_product_ids &lt;chr&gt; 1.5 Timeline The timeline is determined by the data latency provided by NEON. NEON data is released in month long sets, 2 weeks after the month ends. NEON data for a given month is scheduled to be released around the 15th of the following month. Once the NEON data for a previous month is released, teams have between the release of those data to the end of the month to forecast the 7 days of the current month (see table). Forecast submissions will due beginning May 31, 2021 at 11:59 Eastern Standard Time (UTC−05:00) for forecasts that start May 1. Final forecast submissions will be due on August 31, 2021 at 11:59 Eastern Standard Time (UTC−05:00) for forecasts that start August 1. As an example, if NEON water temperature data is released on April 15 for data from March 1 - 31, teams then can use these new March data and the NOAA GEFS forecast issued on April 1 at 00:00 to help generate forecasts from April 1 - April 8 (7 days). This April forecast is due by 11:59 pm EST on April 30. The forecast issue date for the April forecast is April 1, so no new observational data from after that date can be used to constrain forecasts and the forecast should use the weather forecast issued at midnight April 1 (i.e. start of day) as the driver (not the observed meteorology in April or forecasts made at later dates). Evaluation will occur as new NEON data is released. Forecast Timeline Table 1.6 Design Team James Guinnip, Kansas State University Sarah Burnet, University of Idaho Ryan McClure, Virginia Tech Chris Brown, National Oceanic and Atmospheric Administration Cayelan Carey, Virginia Tech Whitney Woelmer, Virginia Tech Jake Zwart, United States Geological Survey 1.7 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge are from the National Ecological Observatory Network (NEON): https://www.neonscience.org/. Scientists from NOAA and USGS have been involved in the design of the challenge. 1.8 Details 1.8.1 Focal variables 1.8.1.1 Surface Mean Daily Dissolved Oxygen Concentration Definition Dissolved oxygen (DO) is the concentration of oxygen dissolved in water. NEON’s 30-minute time resolution from deployed water quality sondes among the freshwater sites reports this concentration as mg L-1. We have adapted the available NEON DO data to output the mean daily DO concentration in mg L-1 from a water quality sonde deployed 1m below the water surface at a lake site (Barco Lake) and a water quality sonde deployed in a stream site (Posey Creek). Common DO concentrations range between 0 and 12 mg L-1 and DO concentrations less than 2 mg L-1 are considered hypoxic. Motivation Dissolved oxygen concentration is a critically important variable in limnology. Forecasts of dissolved oxygen in freshwaters is the first step to understanding other freshwater ecosystem processes. For example, oxygen serves as the gatekeeper to other biogeochemical reactions that occur as well as determine the variety and health of aquatic organisms present in rivers and lakes. Preemptive forecasts of dissolved oxygen concentrations can anticipate periods of high or low oxygen availability, thereby providing insight into how the ecosystem may change at relatively short timescales. 1.8.1.2 Surface Mean Daily Water Temperature Definition Water temperature is the temperature of the water. NEON’s 30-minute time resolution from deployed water temperature sondes in the freshwater sites reports this in degrees celsius (°C). We have adapted the available NEON water temperature data to output the mean daily water temperature in °C from a water temperature sonde deployed 1m below the water surface at a lake site (Barco Lake) and a water temperature sonde deployed in a stream site (Posey Creek). Common water temperatures in lakes and streams range between 4 and 35 °C. Motivation In streams and rivers, forecasting water temperature can be meaningful for protecting aquatic communities while maintaining socio-economic benefits (Ouellet-Proulx et al. 2017). In lentic and lotic systems, successfully forecasting water temperatures can be important for management of fisheries and water utilities that rely on specific threshold temperatures (Zhu et al. 2020). Recently, lake temperature forecasts have been used to predict seasonal turnover, mixing bottom nutrients into the surface and impairing water quality.Data 1.8.2 Focal sites The challenge is focused on two freshwater NEON sites including a lake and stream ecosystem. Table 1 lists the sites. 1.9 References Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 Ouellet-Proulx, S., St-Hilaire, A., and Bouchar, M.-A.. 2017. Water temperature ensemble forecasts: Implementation using the CEQUEAU model on two contrasted river systems. Water 9(7):457. https://doi.org/10.3390/w9070457 Zhu, S., Ptak, M., Yaseen, Z.M., Dai, J. and Sivakumar, B. 2020. Forecasting surface water temperature in lakes: a comparison of approaches. Journal of Hydrology 585, 124809. https://doi.org/10.1016/j.jhydrol.2020.124809 "],["theme-carbon-and-water-fluxes.html", "2 Theme: Carbon and Water Fluxes 2.1 Overview 2.2 Challenge 2.3 Data: Training and Evaluation 2.4 Data: Targets 2.5 Detailed Protocol 2.6 Timeline 2.7 Design team 2.8 Partners 2.9 References", " 2 Theme: Carbon and Water Fluxes What: Net ecosystem exchange of CO2, evapotranspiration, and soil moisture Where: 4 NEON sites that span a water stress gradient in the U.S. The images below show the flux tower at the center of each site. When: Half hour and/or daily forecasts for a 35 day period are submitted once per month January 31-December 31, 2021; later submissions after the January 31 start are permissible Why: Carbon and water cycling are fundimental for climate and water regulation services provided by ecosystems Who: Open to any individual or team that registers How: REGISTER your team and submit forecast We held a Q&amp;A session on January 22, 2021. You can find a recording from that session HERE. 2.1 Overview The exchange of water and carbon dioxide between the atmosphere and the land is akin to earth’s terrestrial ecosystems breathing rate and lung capacity. The water available to plant roots plays a critical role in plant function, and subsequently represents a predominant source of uncertainty for predictions of how much carbon is entering or exiting an ecosystem. One of the best ways to monitor changes in the amount of carbon and water in an ecosystem is the eddy-covariance method. This method observes the net amount of carbon and water entering and exiting ecosystems at half-hourly timesteps, which is important because it can provide information on ecosystem processes such as photosynthesis, respiration, and transpiration, their sensitivities to ongoing climate and land use change, and greenhouse gas budgets for carbon accounting and natural climate solutions. Forecasts of carbon uptake and release, water use, and soil moisture can provide insights into future production of food, fiber, timber, and carbon credits. Additionally, forecasts will highlight the influence that stress and disturbance have on carbon and water cycling. 2.2 Challenge This design challenge asks teams to produce forecasts of net ecosystem exchange of carbon dioxide (NEE), latent heat flux of evapotranspiration (LE), and soil moisture across four NEON sites with differing climates. These target variables are important because they can be used to inform energy budgets and further reduce uncertainty in the CO2 sink or source behavior of the terrestrial biosphere. This forecasting challenge asks teams to forecast NEE, LE, and soil moisture at either the 30-minute or daily time step over the next 35-days using NOAA Global Ensemble Forecast System weather forecasts as drivers (if forecasting model uses meteorological inputs). Monthly forecasts can be submitted for each month in 2021. The challenge will take place using the eddy covariance flux towers at 4 NEON sites: Bartlett Experimental Forest (BART), Konza Prairie Biological Station (KONZ), Ordway-Swisher Biological Station (OSBS), and Santa Rita Experimental Range (SRER). Users are asked to submit their forecast of measured NEON NEE, LE, and soil moisture data, along with uncertainty estimates and metadata. Any NEE, LE, and soil moisture data prior to the month being forecasted may be used to build and improve the models used to generate forecasts. Other data can be used so long as they are not from the month being forecast and the data are made publicly available (minimum of URL, but ideally a script) and accessible to all teams in the challenge. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). 2.3 Data: Training and Evaluation The challenge uses the following NEON data products: DP4.00200.001: Bundled data products - eddy covariance DP1.00094.001: Soil water content and water salinity 2.4 Data: Targets A file with previously released NEON data that has been processed into “targets” is provided below. The same processing will be applied to new data that are used for forecast evaluation. Before the Terrestrial Carbon and Water Flux challenge begins, a processing script will be available in the neon4cast-terrestrial GitHub repository. 2.5 Detailed Protocol Details of the targets, how they are calculated, descriptions of the target files, and examples of other environmental variables that could be used in the Challenge are HERE. Access Targets HERE Download an example of a forecast output format for submission using ensemble members to describe uncertainity HERE 2.6 Timeline The timeline is determined by the data latency provided by NEON. NEON data is released in month long sets, 2 weeks after the month ends. The challenge will begin January 31, 2021 at 11:59 Eastern Standard Time (UTC−05:00) and run through December 31, 2021. Subsequent forecasts are due at 11:59 EST on the final day of each month. NEON data for a given month is scheduled to be released around the 15th of the following month. Once the NEON data for a previous month is released, teams have between the release of those data to the end of the month to forecast the current month. For example, NEON eddy-covariance data area will be released on January 15 that contains values for December 1 - 31. Teams can use these December eddy-covariance data to help generate forecasts from Jan 1 - February 5 (35 days). This January forecast is due by 11:59 pm EST on January 31. The forecast issue date for the January forecast is January 1, so no new observational data from after that date can be used to constrain forecasts and the forecast should use the weather forecast issued at midnight January 1 (i.e. start of day) as the driver (not the observed meteorology in January or forecasts made at later dates). See the calendar below for a visual depiction of the data availability and forecast submission timeline. Evaluation will occur as new NEON data is released. 2.7 Design team Alex Young, SUNY - College of Environmental Science &amp; Forestry George Burba, LI-COR Biosciences Jamie Cleverly, Terrestrial Ecosystem Research Network (TERN) Ankur Desai, University of Wisconsin, Madison Mike Dietze, Boston University Andy Fox, Joint Center for Satellite Data Assimilation William Hammond, Oklahoma State University Danica Lombardozzi, National Center for Atmospheric Research Quinn Thomas, Virginia Tech 2.8 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation-sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge are from the National Ecological Observatory Network (NEON): https://www.neonscience.org/. Ameriflux is an excellent database of eddy-covariance data, including historical data for some of the four challenge sites: https://ameriflux.lbl.gov/. Terrestrial Ecosystem Research Network (TERN) has been involved in the design of the challenge: https://www.tern.org.au/. 2.9 References Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 "],["theme-tick-populations.html", "3 Theme: Tick Populations 3.1 Overview 3.2 Challenge 3.3 Data: Training and Evaluation: 3.4 Data: Targets 3.5 Detailed protocol 3.6 Timeline 3.7 Design team 3.8 Partners 3.9 References", " 3 Theme: Tick Populations What: Amblyomma americanum and Ixodes scapularis nymphal tick abundance per sampled area Where: 22 plots at 7 NEON sites When: Weekly forecasts for 34 weeks into the future starting March 31-October 31, 2021 with training data available January 31, 2021. Forecasts are submitted monthly and later submissions after the March 31 start are permissible. Why: There is a correlation between tick population abundance and disease incidence, meaning forecasts for tick abundance have the potential to aid in our understanding of disease risk through time and space. Who: Open to any individual or team that registers How: REGISTER your team and submit forecast 3.1 Overview Target species for the population forecasts are Amblyomma americanum and Ixodes scapularis nymphal ticks. A. americanum is a vector of ehrlichiosis, tularemia, and southern tick-associated rash illness, while I. scapularis is a vector for Lyme disease, the most prevalent tick-borne disease in North America. Both species are present in the eastern United States, and have been collected at numerous NEON sites. There is a correlation between tick population abundance and disease incidence, meaning forecasts for tick abundance have the potential to aid in our understanding of disease risk through time and space. 3.2 Challenge The challenge is open to any individual, group, or institution that may want to participate. The goals of this challenge are to forecast total Ixodes scapularis and Amblyomma americanum nymphs each epidemiological week (Sun-Sat) per sampled area at a set of NEON plots within NEON sites. Due to challenges in data collected in 2020, this round of the forecasting challenge will simulate a true forecasting challenge by focusing on data from the 2019 field season. NOAA Global Ensemble Forecast System weather forecasts for each NEON site is provided for teams to use: https://data.ecoforecast.org/minio/drivers/noaa/ Teams must provide access (minimum of URL, but ideally a script) to any additional data they wish to use to all teams in the challenge. Teams of various career stages and disciplines are encouraged. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation (PENDING). Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). 3.3 Data: Training and Evaluation: The challenge uses the following NEON data products: DP1.10093.001: Ticks sampled using drag cloths Total Ixodes scapularis will be forecasting for the following plots (siteID_plotID): BLAN_012, BLAN_005, SCBI_013, SCBI_002, SERC_001, SERC_005, SERC_006, SERC_012, ORNL_007 Total Amblyomma americanum will be forecasting for the following plots (siteID_plotID): SCBI_013, SERC_001, SERC_005, SERC_006, SERC_002, SERC_012, KONZ_025, UKFS_001, UKFS_004, UKFS_003, ORNL_002, ORNL_040, ORNL_008, ORNL_007, ORNL_009, ORNL_003, TALL_001, TALL_008, TALL_002 3.4 Data: Targets A file with previously released NEON data that has been processed into “targets” is provided below. The same processing will be applied to new data that are used for forecast evaluation. Before the Tick challenge begins, a processing script will be available in the neon4cast-ticks GitHub repository. 3.5 Detailed protocol Details of the targets, how they are calculated, descriptions of the target files, and examples of other environmental variables that could be used in the Challenge are HERE. Access Targets HERE Download an example of forecast output format for submission HERE 3.6 Timeline The timeline for this challenge will be monthly, which is how often new data will be released by the EFI RCN. The final data set containing the training data will be available no later than January 31st, 2021. The challenge will begin (first forecast submission) on March 31st, 2021 at 11:59 PM Eastern Standard Time, and will run through October 31st, 2021 (last forecast submission). 2019 data will be released on the first of the month following a submission deadline, which gives teams a month to assimilate new data. For example, the forecasts submitted on March 31st, 2021 will be for every epidemiological week starting at the beginning of March 2019 through the end of November 2019. Then, on April 1st, 2021, tick counts from March 2019 will be released. The next forecast submission is April 30th, 2021, which will be for every epidemiological week starting at the beginning of April 2019 through the end of November 2019. The table below shows which epidemiological weeks are to be forecasted for each submission date. 2021 FORECAST SUBMISSION DATE 2019 TARGET EPIDEMIOLOGICAL WEEKS March 31 10-44 April 30 14-44 May 31 19-44 June 30 23-44 July 31 28-44 August 31 32-44 September 31 36-44 October 31 41-44 Evaluation will occur shortly after each forecast submission. 3.7 Design team John Foster, Boston University Matt Bitters, University of Colorado, Boulder Melissa Chen, University of Colorado, Boulder Leah Johnson, Virginia Tech Shannon LaDeau, Cary Institute of Ecosystem Studies Cat Lippi, University of Florida Brett Melbourne, University of Colorado, Boulder Wynne Moss, University of Colorado, Boulder Sadie Ryan, University of Florida 3.8 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge are collected by the National Ecological Observatory Network (NEON; https://www.neonscience.org/). 3.9 References Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 "],["theme-phenology.html", "4 Theme: Phenology 4.1 Overview 4.2 Challenge 4.3 Data: Training and Evaluation: 4.4 Data: Targets 4.5 Detailed protocol 4.6 Timeline 4.7 Design team 4.8 Partners 4.9 References", " 4 Theme: Phenology What: Terrestrial phenology defined by daily greenness of plants Where: 8 deciduous broadleaf forest NEON sites in the continental U.S. When: Daily forecasts for 35-days in the future from February 1 - July 1, 2021 with a second round being run for the autumn. Forecast submissions are accepted daily, and later submissions after the February 1 start are permissible. Why: Phenology has been identified as one of the primary ecological fingerprints of global climate change. Who: Open to any individual or team that registers How: REGISTER your team and submit forecast We held a Q&amp;A session on January 27, 2021. You can find a recording from that session HERE. 4.1 Overview Phenology has been shown to be a robust integrator of the effects of year-to-year climate variability and longer-term climate change on natural systems (e.g., recent warming trends). Experimental studies have shown how other global change factors (e.g., elevated CO2 and N deposition) can also influence phenology. There is a need to better document biological responses to a changing world, and improved phenological monitoring at scales from individual organisms to ecosystems, regions, and continents will contribute to achieving this goal. Phenology researchers often use digital cameras (such as those that are part of the PhenoCam Network) that take regular repeated images of plant canopies to monitor changes in greenness throughout the year. The PhenoCam Network is a cooperative continental-scale phenological observatory that uses digital repeat photography to track vegetation phenology in a diverse range of ecosystems across North America and around the World. Imagery and data are made publicly available in near-real-time through the PhenoCam webpage: http://phenocam.sr.unh.edu/. 4.2 Challenge This is an open ecological forecasting challenge to forecast spring green-up of the common greenness index (GCC), as measured by digital cameras at various deciduous broadleaf NEON sites. The forecasts will be forecasts of daily mean GCC (specifically the 90% quantile, which has been shown to be more robust). The sites include Harvard Forest (HARV), Bartlett Experimental Forest (BART), Smithsonian Conservation Biology Institute, (SCBI), Steigerwaldt Land Services (STEI), The University of Kansas Field Station, KS (UKFS), Great Smoky Mountains National Park (GRSM), Dead Lake (DELA), and National Grassland (CLBJ). NOAA Global Ensemble Forecast System weather forecasts for each NEON site is provided for teams to use: https://data.ecoforecast.org/minio/drivers/noaa/ Teams must provide access (minimum of URL, but ideally a script) to any additional data they wish to use to all teams in the challenge. Teams of various career stages and disciplines are encouraged to submit forecasts. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation (PENDING). Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). 4.3 Data: Training and Evaluation: The challenge uses the following NEON data products: DP1.00033.001: Phenology images 4.4 Data: Targets A file with previously released NEON data that has been processed into “targets” is provided below. The same processing will be applied to new data that are used for forecast evaluation. Before the Phenology challenge begins, a processing script will be available in the neon4cast-phenology GitHub repository. 4.5 Detailed protocol Details of the targets, how they are calculated, descriptions of the target files, and examples of other environmental variables that could be used in the Challenge are HERE. Access Targets HERE Download an example of a forecast output format for submission in netcdf format using ensemble members is here: HERE Download an example of a forecast output format for submission in csv format using summary statistics is here: HERE 4.6 Timeline Forecasts for a minimum of 35 days can be submitted daily by 6 pm ET for the period of February 1st through July 1st, 2021. Forecast should be submitted starting February 1st by 6 pm ET. A minimum of 35 days in the future must be forecasted for each submission. For example, the first submitted forecast should be for at least February 1st – March 7th, but it could be for the full spring. New forecasts can be submitted daily as new weather forecasts and observations (e.g., PhenoCam) become available. Processed PhenoCam data will be available daily by 11:59 pm ET for each day. Teams are allowed to start submitting forecasts after February 1st, but only forecasts of future days (when submitted) will be allowed. Late forecasts might be allowed under extenuating circumstances related to computer failure or processing delayed on our end. Forecasts do not have to be submitted daily and can be longer than 35 days. 4.7 Design team Kathryn Wheeler, Boston University Michael Dietze, Boston University Kathy Gerst, National Phenology Network Chris Jones, NC State University Andrew Richardson, Northern Arizona University Bijan Seyednasrollah, Northern Arizona University, PhenoCam Network 4.8 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge are collected by the National Ecological Observatory Network (NEON; https://www.neonscience.org/) and hosted by the Phenocam Network (http://phenocam.sr.unh.edu/). The forecasting challenge was developed in collaboration with the USA National Phenology Network: https://www.usanpn.org/usa-national-phenology-network. 4.9 References Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 "],["theme-beetle-communities.html", "5 Theme: Beetle Communities 5.1 Overview 5.2 Challenge 5.3 Data: Training and Evaluation: 5.4 Data: Targets 5.5 Detailed protocol 5.6 Timeline 5.7 Design team 5.8 Partners 5.9 References", " 5 Theme: Beetle Communities What: Beetle abundance and species richness from the National Ecological Observatory Network Where: 47 terrestrial NEON sites that span the diverse ecosystems of the U.S. When: Weekly forecasts for 20 months in the future starting June 30-December 31, 2021; later submissions after the June 30 start are permissible Why: Improve understanding of habitat quality, conservation potential, land-use sustainability, and biodiversity change in response to global change and ecological disturbances Who: Open to any individual or team that registers How: REGISTER your team and submit forecast 5.1 Overview Biodiversity monitoring is critical for understanding environmental quality, evaluating the sustainability of land-use practices, and forecasting future impacts of global change on ecosystems. Sentinel species give forewarning of environmental risk to humans, so are particularly useful for such monitoring and forecasting efforts because they can provide surrogates for other co-located components of biodiversity (Sauberer et al. 2004). Ground beetles (Family: Carabidae) are appropriate candidates for biodiversity monitoring and ecological forecasting as they are well-studied sentinel species that are geographically widespread, and their community dynamics are particularly congruent with the diversity of other invertebrates (Holland 2002; Lundgren &amp; McCravy 2011; Bousquet 2012; Hoekman et al. 2017). Therefore, monitoring carabid communities and forecasting changes in their species richness and abundance can be useful in studying edge effects and habitat quality (Magura 2002), conservation potential (Butterfield 1995), land-use sustainability (Pearce &amp; Venier 2006) and biodiversity change in response to global change and ecological disturbances (Koivula 2011). Most ecological forecasting models are limited in the geographic scale and also suffer from scarcity of temporally extensive data. Further, most existing forecasting efforts focus on a single species (Humphries et al. 2018) with limited community-wide forecasts at the continental scale. Developing forecasts for community-scale metrics (i.e., species richness, abundance) and evaluating such models for accuracy and generalizability can help test our scientific knowledge of spatial (geographical turnover) and temporal (seasonal, inter-annual) carabid community dynamics (Dietze et al. 2018). Such forecasting models can inform regional or local habitat management, identify where biodiversity monitoring efforts should be prioritized, and shed light on what data or modelling techniques are needed to build the best forecasts of ecological dynamics (e.g., can we predict richness or abundance better and why?) (Johansson et al. 2019). With the long-term, community-wide, continental-scale data collection through the National Ecological Observatory Network (NEON), 181 data products are available for 81 sites in the US (47 terrestrial, at which carabids are sampled, and 34 aquatic). Fully initiated in 2019, this sampling will continue for 30 years (Schimel et al. 2007; 2011). NEON has effectively removed the previous barriers to community-scale forecasting across a broader geographical realm. 5.2 Challenge This is an open ecological forecasting challenge to forecast carabid species richness, defined as the total number of species, and abundance, defined as the total number of carabid individuals. The forecasts should be done weekly per site for all NEON terrestrial sites with richness being absolute and abundance scaled by the sampling effort. Contributing teams are required to submit a forecast for May-Dec 2021 and Jan-Dec 2022 on June 30, 2021. However, teams are encouraged to update their forecasts on the last day of each month, ending on December 31, 2021, as NEON validation data are released. NEON releases carabid sampling data weekly and no sooner than 60 days after collection, so a model submitted on June 30 can include a forecast for the first week of May, and so forth. Teams may use any open data products as drivers of richness and abundance so long as they are not from the month being forecast, and are made publicly available (minimum of URL, but ideally a script). Potential driver data sources include: NEON site data (Soil and sediment data, Terrestrial Plant data, weather data), NOAA forecasts, and beyond. Submissions of forecast and metadata will be through https://data.ecoforecast.org/minio/submissions/ using prescribed file formats described in the challenge theme documentation (PENDING). Forecasts will be scored and compared using the Continuous Ranked Probability Score, a metric that combines accuracy and uncertainty estimation (Gneiting, T., &amp; Raftery, A. E., 2007). Only weeks for which data are collected will be included in scoring. 5.3 Data: Training and Evaluation: The challenge uses the following NEON data product: DP1.10022.001: Ground beetles sampled from pitfall traps 5.4 Data: Targets A file with previously released NEON data that has been processed into the aggregate “target” variables (richness and abundance) is provided below. The same processing will be applied to new data that are used for forecast evaluation. Further information about data structure, initial code for processing, and example null forecasts is provided in the neon4cast-beetles GitHub repository. 5.5 Detailed protocol Details of the targets, how they are calculated, descriptions of the target files, and examples of other environmental variables that could be used in the Challenge are HERE. Access Targets HERE Download an example of a forecast output format for submission HERE 5.6 Timeline The timeline is determined by the data latency provided by NEON. NEON carabid pitfall trap data is released weekly with a latency of at least 60 days after collection. Weekly forecasts were chosen to best match up with NEON’s weekly release of carabid data. The challenge will begin June 30, 2021 at 11:59 Eastern Standard Time (UTC−05:00) and run through December 31, 2021. Subsequent forecasts are due at 11:59 EST on the final day of each month. As an example, carabid pitfall trap data released in the last week of June would include data as recent as the last week of April. Thus, a model submitted in the last week of June could include forecasts from May onwards. Then, the forecast update submitted at the end of July can use new May data to help refine June forecasts, or forecasts following June, depending on what drivers are used. The July forecast update is due by 11:59 pm EST on July 31. Forecasts updates will not be considered for weeks where NEON validation data has already been released (i.e., no May forecast updates may be submitted on July 31) or for weeks when no NEON carabid data is available. Pitfall traps are collected and reset every 2 weeks throughout the growing season. Due to weather and conditions beyond the field team’s control, this collection schedule may not be followed. Thus, the exact collection dates cannot be known until they have happened. Field data are made publicly available on the data portal as the bet_fielddata dataframe no sooner than 14 days after collection. Teams can use bet_fielddata to inform the actual collection schedule. Data with parataxonomist identifications are made publicly available on the data portal as the bet_sorting dataframe no sooner than 60 days after collection. Data are released on the NEON data portal weekly, but may be released on any day of the week. The forecast schedule follows the ISO week standard with weeks starting on Mondays. 5.7 Design team Anna Spiers, University of Colorado, Boulder Carl Boettiger, University of California, Berkeley Tad Dallas, Louisiana State University Nico Franz, NEON Biorepository at Arizona State University Kari Norman, University of California, Berkeley Thilina Surasinghe, Bridgewater State University Brett Melbourne, University of Colorado, Boulder Eric Sokol, NEON Kelsey Yule, NEON Biorepository at Arizona State University 5.8 Partners The challenge is hosted by the Ecological Forecasting Initiative (EFI; https://ecoforecast.org/) and its U.S. National Science Foundation sponsored Research Coordination Network (EFI-RCN; https://ecoforecast.org/rcn/). Data used in the challenge from National Ecological Observatory Network (NEON): https://www.neonscience.org/. 5.9 References Bousquet, Y. (2012) Catalogue of Geadephaga (Coleoptera: Adephaga) of America, north of Mexico. ZooKeys 245: 1-1722. https://doi.org/10.3897/zookeys.245.3416 Butterfield, J., Luff, M., Baines, M., Eyre, M. (1995) Carabid beetle communities as indicators of conservation potential in upland forests. Forest Ecology and Management 79, 63-77. https://doi.org/10.1016/0378-1127(95)03620-2 Dietze, M.C., Fox, A., Beck-Johnson, L.M., Betancourt, J.L., Hooten, M.B., Jarnevich, C.S., Keitt, T.H., Kenney, M.A., Laney, C.M., Larsen, L.G. (2018) Iterative near-term ecological forecasting: Needs, opportunities, and challenges. Proceedings of the National Academy of Sciences 115, 1424-1432. https://doi.org/10.1073/pnas.1710231115 Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437 Hoekman, D., LeVan, K.E., Gibson, C., Ball, G.E., Browne, R.A., Davidson, R.L., Eriwin, T.L., Knisley, C.B., LaBonte, J.R., Lundgren, J., Maddison, D.R., Moore, W., Niemela, J., Ober, K.A., Pearson, D.L. Spence, J.R., Will, K., Work, T. (2017) Design for ground beetle abundance and diversity sampling within the National Ecological Observatory Network. Ecosphere, 8(4), e01744. https://doi.org/10.1002/ecs2.1744 Holland, J.M. (2002) The agroecology of carabid beetles. Intercept Limited, Andover. Humphries, G.R., Che-Castaldo, C., Bull, P., Lipstein, G., Ravia, A., Carrión, B., Bolton, T., Ganguly, A., Lynch, H.J. (2018) Predicting the future is hard and other lessons from a population time series data science competition. Ecological Informatics 48, 1-11. https://doi.org/10.1016/j.ecoinf.2018.07.004 Johansson, M.A., Apfeldorf, K.M., Dobson, S., Devita, J., Buczak, A.L., Baugher, B., Moniz, L.J., Bagley, T., Babin, S.M., Guven, E. (2019) An open challenge to advance probabilistic forecasting for dengue epidemics. Proceedings of the National Academy of Sciences 116, 24268-24274. https://doi.org/10.1073/pnas.1909865116 Koivula, M.J. (2011) Useful model organisms, indicators, or both? Ground beetles (Coleoptera, Carabidae) reflecting environmental conditions. ZooKeys, 287-317. https://doi.org/10.3897/zookeys.100.1533 Lundgren, J., McCravy, K. (2011) Carabid beetles (Coleoptera: Carabidae) of the Midwestern United States: A review and synthesis of recent research. Terrestrial arthropod reviews 4, 63-94. https://doi.org/10.1163/187498311X565606 Magura, T. (2002) Carabids and forest edge: spatial pattern and edge effect. Forest Ecology and Management 157, 23-37. https://doi.org/10.1016/S0378-1127(00)00654-X Pearce, J.L., Venier, L.A. (2006) The use of ground beetles (Coleoptera: Carabidae) and spiders (Araneae) as bioindicators of sustainable forest management: A review. Ecological Indicators 6, 780-793. https://doi.org/10.1016/j.ecolind.2005.03.005 Sauberer, N., Zulka, K.P., Abensperg-Traun, M., Berg, H.-M., Bieringer, G., Milasowszky, N., Moser, D., Plutzar, C., Pollheimer, M., Storch, C. (2004) Surrogate taxa for biodiversity in agricultural landscapes of eastern Austria. Biological Conservation 117, 181-190. https://doi.org/10.1016/S0006-3207(03)00291-X Schimel, D., Hargrove, W., Hoffman, F., MacMahon, J. (2007) NEON: a hierarchically designed national ecological network. Frontiers in Ecology and the Environment 5, 59-59. https://doi.org/10.1890/1540-9295(2007)5[59:NAHDNE]2.0.CO;2 Schimel, D., Keller, M., Berukoff, S., Hufft, R., Loescher, H., Powell, H., Kampe, T., Moore, D., Gram, W. (2011) NEON Science Strategy: Enabling Continental-Scale Ecological Forecasting. https://www.neonscience.org/sites/default/files/basic-page-files/NEON_Strategy_2011u2_0.pdf "],["participation.html", "6 Participation 6.1 Participation guidance 6.2 Submission instructions 6.3 Participation agreement 6.4 NEON Data Use 6.5 Additional data options", " 6 Participation 6.1 Participation guidance 6.1.1 How to participate Participation requires that teams: Complete a REGISTRATION for each forecast theme you are participating in and each model you are contributing within a theme Agree to the participation agreement below Submit forecast netCDF or csv file(s) Provide the metadata xml file documenting the forecast One contact person should register on behalf of their team. That contact person will be asked to provide the group members’ names, emails, and affiliations so that everyone in the group can receive an invitation to join the Challenge theme Slack channel and access group resources. Teams are allowed and encouraged to join the challenge after the start date of each Challenge theme because there are multiple deadlines to submit forecasts. However, only forecasts submitted by each submission deadline will be officially scored. 6.1.2 Teams Teams can be individuals or groups. They can represent institutions or organizations. You will have 25 characters for a team name (e.g., “EFI Null Model”) and 10 characters for the team name ID (no spaces allowed; e.g., EFI_Null). The registration includes team categories (e.g., undergraduate only, graduate only, multi-institution, etc). Please check all that apply. If your team wants to submit multiple forecasts, please register a team for each model as only one forecast model per cycle per team is allowed. If there are two different time steps in a challenge theme (e.g., the terrestrial carbon flux theme has a 30-minute and 1-day option), register each as separate teams. 6.1.3 Slack and GitHub Communication We strongly encourage participants to use the Challenge theme Slack channels to ask questions, discuss ideas and challenges, and share resources. Overall, we strongly encourage a collegial approach to the Challenge – this is a friendly competition to move the field forward and bring more people into the community, not a cutthroat competition to win by denying other teams useful information. GitHub repositories for each Challenge theme will be available with helper code and an example workflow (null models). We encourage teams to contribute code to these repositories (via Pull Request) if they develop additional helper code. This is especially important if an individual or group is going to add additional data constraints to their forecast. Remember, the use of data external to NEON is allowed and encouraged so long as it is publicly available and other teams are notified about it. Also, while most anything could be used to calibrate parameters and constrain initial conditions, only other forecasts (e.g. weather) can be used as drivers/covariates during the actual forecast period. 6.2 Submission instructions 6.2.1 Forecast format Teams will submit their forecasts as a single netCDF or csv file with the following naming convention: theme_name-year-month-day-team_name_ID.csv or theme_name-year-month-day-team_name_ID.nc Where year, month, and day are the year, month, and day for the first day of the submitted forecast and the team_name_ID is the code for the team name that is specified in the registration (team_name_ID is a 10 character name with no spaces in it). The theme_name options are: terrestrial_daily, terrestrial_30min, aquatics, beetles, ticks, or phenology. Forecast netCDF or csv files should have the following columns (csv) or variables (netcdf) that correspond to the columns. 6.2.1.1 Terrestrial time: YYYY-MM-DD HH:MM UTC of the start of the 30-minute value or YYYY-MM-DD for daily forecasts siteID: NEON code for site ensemble*: integer value for forecast replicate within the year and month (i.e. ensemble member or MCMC sample) forecast: set as 1 for each row (1 = variables were forecasted; a 0 would designate a hindcast which does not apply to submissions to the challenge) data_assimilation: set as 0 for each row (0 = no data assimilation occurred because it is a forecast) nee: net ecosystem exchange (umol CO2 m-2 s-1) le: latent heat (W m-2) vswc: volumetric soil water content (%) 6.2.1.2 Beetles time: YYYY-MM-DD of forecast (where the DD is the first day of the week that is forecasted) siteID: NEON code for site ensemble*: integer value for forecast replicate within the year and month (i.e. ensemble member or MCMC sample) forecast: set as 1 for each row (1 = variables were forecasted; a 0 would designate a hindcast which does not apply to submissions to the challenge) data_assimilation: set as 0 for each row (0 = no data assimilation occurred because it is a forecast) abundance: abundance of beetles richness: species richness of beetles 6.2.1.3 Aquatics time: YYYY-MM-DD of forecast siteID: NEON code for site ensemble*: integer value for forecast replicate within the year and month (i.e. ensemble member or MCMC sample) forecast: set as 1 for each row (1 = variables were forecasted; a 0 would designate a hindcast which does not apply to submissions to the challenge) data_assimilation: set as 0 for each row (0 = no data assimilation occurred because it is a forecast) oxygen: dissolved oxygen (ug/L) temp: water temperature (C) Here is an example of a forecast file that meets the standard for the aquatics theme readr::read_csv(&quot;https://data.ecoforecast.org/forecasts/aquatics/aquatics-2021-03-01-EFInull.csv.gz&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## time = col_date(format = &quot;&quot;), ## ensemble = col_double(), ## siteID = col_character(), ## oxygen = col_double(), ## temperature = col_double(), ## obs_flag = col_double(), ## forecast = col_double(), ## data_assimilation = col_double() ## ) ## # A tibble: 28,000 x 8 ## time ensemble siteID oxygen temperature obs_flag forecast ## &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-03-01 1 BARC 8.98 19.1 2 1 ## 2 2021-03-01 2 BARC 8.67 21.6 2 1 ## 3 2021-03-01 3 BARC 8.73 28.9 2 1 ## 4 2021-03-01 4 BARC 8.12 28.2 2 1 ## 5 2021-03-01 5 BARC 8.18 17.6 2 1 ## 6 2021-03-01 6 BARC 8.70 34.1 2 1 ## 7 2021-03-01 7 BARC 8.83 14.9 2 1 ## 8 2021-03-01 8 BARC 8.25 23.7 2 1 ## 9 2021-03-01 9 BARC 8.47 15.0 2 1 ## 10 2021-03-01 10 BARC 8.68 26.3 2 1 ## # … with 27,990 more rows, and 1 more variable: data_assimilation &lt;dbl&gt; 6.2.1.4 Phenology time: YYYY-MM-DD siteID: NEON code for site ensemble*: integer value for forecast replicate within the year and month (i.e. ensemble member or MCMC sample) forecast: set as 1 for each row (1 = variables were forecasted; a 0 would designate a hindcast which does not apply to submissions to the challenge) data_assimilation: set as 0 for each row (0 = no data assimilation occurred because it is a forecast) gcc_90: green chromatic coordinate Here is an example of a forecast file that meets the standard for the phenology theme 6.2.1.5 Ticks time: YYYY-MM-DD of forecast (where the DD is the first day of the week that is forecasted) siteID: NEON code for site plotID: NEON plotID ensemble*: integer value for forecast replicate within the year and month (i.e. ensemble member or MCMC sample) forecast: set as 1 for each row (1 = variables were forecasted; a 0 would designate a hindcast which does not apply to submissions to the challenge) data_assimilation: set as 0 for each row (0 = no data assimilation occurred because it is a forecast) amblyomma_americanum: Number of Amblyomma americanum nymphs per plot per week ixodes_scapularis: Number of Ixodes scapularis nymphs per plot per week *Teams that are NOT using ensemble-based forecast methods should replace the ensemble column with a statistic column. Multiple statistics can be reported using a long format in a csv or adding a statistic dimension in netCDF. The required options for this column are mean and sd (standard deviation). You can also include Conf_interv_02.5, Conf_interv_97.5, Pred_interval_02.5, and Pred_interval_97.5 to describe uncertainty but these are optional. The numbers in the last four options indicate equal-tail quantiles for a 95% interval estimate and Conf_=confidence and Pred_=predictive. If statistics are reported we will make a Gaussian assumption when calculating error scores. The Continuous Ranked Probability Score is based on the predictive distribution so reported sd should be for the predictive distribution. For those using netCDF, the order of dimensions on forecast variables should be: time, site, plot [ticks only], and ensemble. In practice using a netCDF for the forecast forecast with the statistic rather than ensemble is not ideal. Additional detail about file formats can be found in the EFI Forecast Standard Documentation. 6.2.2 Metadata format Each submission requires a metadata file to be submitted. The metadata file must have the same name as the submitted forecast, but with the .xml extension. Model descriptions can be uploaded to https://data.ecoforecast.org/minio/submissions/ theme_name-year-month-day-team_name_ID.xml The metadata standard has been designed by the Ecological Forecasting Initiative and is build off the widely used Ecological Metadata Language (EML). The following components are required: - Title - pubDate: date that forecast is generated - License: See below for details - Creator(s) - Spatial and temporal coverage - File descriptions - Model timestep - forecast_horizon (length) - forecast_issue_time - forecast_iteration_id - forecast_project_id: team_name_ID (e.g., EFI_NULL) - model_description which includes: forecast_model_id, model name, type (statistical, process-based, machine-learning, etc), and repository (url or DOI). - Info on model structure and uncertainty (standards section 2.1.2) The license for the forecast output is required to be from the following Creative Commons License options: CC BY, CC BY-SA, CC BY-NC, CC BY-NC-SA. While we recommend a CC BY license, teams may use less permissive CC licenses if more appropriate. The license entry can be the CC option (i.e., CC BY) and a web link to the full CC license (e.g., https://creativecommons.org/licenses/by/4.0/) We recommend teams read the full metadata standard description for definitions and more information, and in particular that they look at the example vignettes, which demonstrate the standard being used. Note that these Standards are a work in progress. If you find issues as you are applying them, let us know at eco4cast.initaitive@gmail.com. The Ecological Forecasting Initiative has provided R scripts to assist in generating the metadata XML file. The scripts can be found at the GitHub repository for the standard: https://github.com/eco4cast/EFIstandards as well as the EML validator. Teams are encouraged to check the validity of their metadata before submission. 6.2.3 Submission process Individual forecast (csv, netCDF) and metadate (xml) files can be uploaded any time before the specific deadlines as defined by each theme. Only the most recent files will be scored. Teams will submit their forecast netCDF or csv files through the challenge website. You can manually submit your forecast through the https://data.ecoforecast.org/minio/submissions/ website using the red plus on the bottom left. You can submit from an R script using the following: Sys.setenv(&quot;AWS_DEFAULT_REGION&quot; = &quot;data&quot;, &quot;AWS_S3_ENDPOINT&quot; = &quot;ecoforecast.org&quot;) aws.s3::put_object(object = &quot;theme_name-forecast-year-month-day-team_name.csv&quot;, bucket = &quot;submissions&quot;) Submissions need to adhere to the forecast format that is provided above, including the file naming convention. Our cyberinfastructure automatically evaluates forecasts and relies on the expected formatting. Contact eco4cast.initiative@gmail.com if you experience technical issues with submitting. Note: If you have used AWS in the past you might have credential files in an .aws folder in your home directory that will cause an error when you try to upload to a non-AWS bucket. If you encounter this error you may need to rename your credentials files so put_object doesn’t try to read them. 6.2.4 Archiving models Teams are highly encouraged to publicly archive the code they are using for their forecast models and workflows. Information about where models are archived would be included in your metadata XML. Teams are also encouraged to use Docker or Singularity to containerize their models &amp; workflows. EFI conventions for containerizing forecasts are still being developed, but our aim (particularly in later years of the forecast challenge) is to be able to provide shared cyberinfrastructure that makes it easier for teams to automate containerized forecasts. Containers will also facilitate Challenge themes interested in performing post-hoc analyses, such as uncertainty quantification and sensitivity analysis. 6.2.5 Computational reesources We are currently working with CyVerse for access to computational resources for teams that require resources not available through home institutions. We will update with more details as they become available. 6.3 Participation agreement All participants agree to have their forecast posted in real-time on the NEON Ecological Forecast Challenge Output RShiny app (in development) and potentially published in a scientific journal. The manuscripts describing the accuracy of forecasts across teams will be coordinated by the Ecological Forecasting Initiative Research Coordination Network and extend authorship to members of each team with an opt-in policy. If a publication is generated by a forecast team, we ask that the manuscript acknowledge the Ecological Forecasting Initiative Research Coordination Network and its support from the National Science Foundation (DEB-1926388). 6.4 NEON Data Use NEON data products, software, and derivatives thereof are freely available for use when accompanied by appropriate disclaimers, acknowledgments, and data citations, defined in the NEON data use policy. 6.5 Additional data options Individuals and groups may create forecasts that use other publicly available data in addition to the NEON data, so long as other teams participating in the challenge are notified about the existence of the data via the Challenge theme’s Slack channel. Teams are encouraged to make available the code they are using to access, download, and process any additional data constraints they are using, ideally via a pull request to each Challenge Github repo. When considering the use of data in forecasts it is important to distinguish data that are being used as drivers/covariates during each forecast from data being used to constrain model structure, parameters, initial conditions, and error distributions. While the latency of NEON data requires that some of our forecast will be (fully or partly) hindcasts, all forecasts should be run as if they are true forecasts – you cannot use any observed data as a driver/covariate or constraint during the forecast period itself as that info would not have been available at the forecast start date. For example, if you find that a particular variable is a useful covariate during the model development &amp; calibration period (e.g. soil temperature) then you would need to find or make a forecast of that variable if you want to use it as a covariate. Teams using meteorological covariates should use the shared meteorological driver data provided by EFI (see Shared Forecast Drivers). As an example of potentially useful external data, each NEON site has subsets of various remote sensing products that are hosted on the ORNL DAAC (ORNL DAAC subsets). These include: MODIS collection 6: LAI, FPAR, burned area, surface reflectance, land surface temperature, vegetation indices (NDVI, EVI), modeled ET, GPP, NPP. VIIRS collection 1: surface reflectance, vegetation indices, LAI, FPAR, land surface temperature, SMAP: modeled NEE, GPP, Rh, SOC Daymet: daily surface weather data "],["shared-forecast-drivers.html", "7 Shared Forecast Drivers 7.1 Meteorology: NOAA Global Ensemble Forecasting System 7.2 Meteorology: NEON Observed", " 7 Shared Forecast Drivers We are downloading, subsetting, and processing forecasted meteorology drivers for each NEON site. Currently, we have NOAA’s Global Ensemble Forecasting System V12 output available at the 1 hr time resolution for each NEON site. For forecasts generated at midnight (00) UTC, the forecasts extend 35-days in the future. For forecasts generated at 06, 12, and 18 UTC, the forecasts extend 16-days in the future. There are 31 ensemble members for each forecast. Each ensemble member is available as a separate netcdf file. The following meteorological variables are included: air temperature, air pressure, wind speed, precipitation, downwelling longwave radiation, downwelling shortwave radiation, relative humidity, specific humidity, and total cloud cover. The weather forecasts are available through an s3 bucket (see Meteorology: NOAA Global Ensemble Forecasting System below) with multiple ways to access them: You can click on a file in the browser, you can directly download individual files from the command line using the file address, or you can download multiple files using aws.s3 commands. We provide an example using the aws.s3 package in R for downloading all the ensemble members for particular location, forecast cycle (00, 06, 12, or 18), and NEON site at: https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/download_noaa_files_s3.R Additionally we provide example code in R for converting the netcdf files to csv files https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/noaa_gefs_read_example.R https://github.com/eco4cast/neon4cast-shared-utilities/blob/main/noaa_gefs_read.R 7.1 Meteorology: NOAA Global Ensemble Forecasting System 1 Hour NOAA Drivers 7.2 Meteorology: NEON Observed In development through collaboration with NEON and NCAR. "],["evaluation.html", "8 Evaluation 8.1 Results 8.2 Scoring Metric: Continuous Ranked Probability Score 8.3 Null forecast 8.4 Forecast Submission Visualization and Leaderboard", " 8 Evaluation Forecasts will be evaluated at each site and forecast horizon (i.e., time-step into the future), and a summary score will be assigned evaluating overall performance of all forecast submissions across sites. Forecasts will also be compared to a null model. Forecast evaluation results will be presented for all submitted models together and separately for each team category: undergraduate student only team, graduate student only team, post-doc only team, single institution team, multi-institution team, international team (team with individuals from at least two countries). 8.1 Results Preliminary results will be distributed using the NEON Ecological Forecast Challenge Output RShiny app and at https://data.ecoforecast.org/minio/scores/. We intend to write a joint manuscript synthesizing forecasts. Teams are welcome to publish results from their model at any time. If a publication is generated we encourage the manuscript to acknowledge the Ecological Forecasting Research Coordination Network and its support from the National Science Foundation (DEB-1926388). 8.2 Scoring Metric: Continuous Ranked Probability Score Forecasts will be scored using the continuous ranked probability score (CRPS), a proper scoring rule for evaluating forecasts presented as distributions or ensembles (Gneiting &amp; Raftery 2007). The CRPS compares the forecast probability distribution to that of the validation observation and assigns a score based on both the accuracy and precision of the forecast. We will use the ‘crps_sample’ function from the scoringRules package in R to calculate the CRPS for each forecast. We will generate a combined score for all locations and forecast horizons. Forecasts will also be evaluated using the CRPS at each time-step in the forecast horizon and each location included in the forecasts. 8.2.1 Example of a CRPS calculation from an ensemble forecast The following uses Equation 2 in Jordan, Kruger, and Lerch 2018 Equation 1 from Jordan, Kruger, and Lerch 2018. First, create a random sample from a probability distribution. This is the “forecast” for a particular point in time. For simplicity, we will use a normal distribution with a mean of 8 and standard deviation of 1 x &lt;- rnorm(1000, mean = 8, sd = 1.0) Second, we have our data point (i.e., the target). We will set it to zero as well y &lt;- 8 Now calculate CRPS using Equation 2 s &lt;- 0 for(i in 1:length(x)){ for(j in 1:length(x)){ s &lt;- s + abs(x[i] - x[j]) } } crps_equation_2 &lt;- mean(abs(x - y)) - s / (2 * length(x)^2) crps_equation_2 ## [1] 0.2404916 Now calculate using the crps_sample() function in the scoringRules package crps_sample(y = y, dat = x) ## [1] 0.2404916 8.2.2 Exploring the scoring surface Now lets see how the CRPS changes as the mean and standard deviation of the forecasted distribution change First, set vectors for the different mean and SD values we want to explore sample_mean &lt;- seq(4, 12, 0.1) sample_sd &lt;- seq(0.1, 10, 0.1) Second, set our observed value to 8 for simplicity y &lt;- 8 Now calculate the CRPS at each combination of forest mean and SD combined &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd))) for(i in 1:length(sample_mean)){ for(j in 1:length(sample_sd)){ sample &lt;- rnorm(10000, sample_mean[i], sample_sd[j]) combined[i, j] &lt;- crps_sample(y = y, dat = sample) } } Finally, visualize the scoring surface with the observed value represented by the red line contour(x = sample_mean, y = sample_sd, z = as.matrix(combined),nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) The contour surface highlights the trade-off between the mean and standard deviation. 8.2.3 CRPS from the Normal Distribution If the distributional forecast is a normal distribution represented by a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), an ensemble of predictions is not needed to evaluate CRPS because we can take advantage of the analytic solution to CRPS under the normal assumption (Equation 4 from Calibrated Probabilistic Forecasting Using Ensemble Model Output Statistics and Minimum CRPS Estimation). Equation 5 from Calibrated Probabilistic Forecasting Using Ensemble Model Output Statistics and Minimum CRPS Estimation gives \\[\\begin{align*} CRPS(N(\\mu, \\sigma^2) | y) = \\sigma \\left( \\frac{y - \\mu}{\\sigma} \\left( 2 \\Phi\\left( \\frac{y - \\mu}{\\sigma} \\right) - 1 \\right) + 2 \\phi \\left( \\frac{y - \\mu}{\\sigma} \\right) - \\frac{1}{\\sqrt{\\pi}} \\right) \\end{align*}\\] for \\(\\Phi(\\cdot)\\) and \\(\\phi(\\cdot)\\) the standard normal CDF and PDF, respectively. Therefore, if the forecast distribution is truly a normal distribution (often this isn’t true in forecasts that only report a mean and sd) a simplified score can be applied as follows: sample_mean &lt;- seq(4, 12, 0.1) sample_sd &lt;- seq(0.1, 10, 0.1) combined_norm &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd))) for(i in 1:length(sample_mean)){ for(j in 1:length(sample_sd)){ combined_norm[i, j] &lt;- crps_norm(y = y, mean = sample_mean[i], sd = sample_sd[j]) } } Finally, visualize the scoring surface with the observed value represented by the red line contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) Note that at a given value of the sd, the lowest score is achieved at \\(\\mu = y\\) as shown for each of the blue lines where the minmum value of the score across each blue line is at the red line. This behavior make sense because the CRPS is a score that reward accuracy and precision. Thus, for any given level of precision (represented by the standard deviation), CRPS is optimized by producing the most accurate prediction of the distribution’s location. contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;) abline(v = y, col = &quot;red&quot;) abline(h = 2.5, col = &quot;blue&quot;) abline(h = 4.3, col = &quot;blue&quot;) abline(h = 6.8, col = &quot;blue&quot;) Interestingly, for a given mean \\(\\mu \\neq y\\) we find a pattern that makes intuitive sense given the goal of CRPS to produce forecasts that are both accurate and precise. For a given amount of bias in the prediction (i.e., given a \\(\\mu \\neq y\\)), the optimal score is achieved by a standard deviation that slightly larger than the bias layout(matrix(1:4, 2, 2, byrow = TRUE)) ## plots for mu = 7 mu &lt;- 7 contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;, main = paste0(&quot;CRPS contour given mu = &quot;, mu)) abline(v = mu, col = &quot;red&quot;) min_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))] abline(h = min_sd, col = &quot;blue&quot;) plot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = &#39;l&#39;, main = paste0(&quot;CRPS profile given mu = &quot;, mu)) abline(v = min_sd, col = &quot;blue&quot;) ## plots for mu = 11 mu &lt;- 11 contour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = &quot;Mean&quot;, ylab = &quot;SD&quot;, main = paste0(&quot;CRPS contour given mu = &quot;, mu)) abline(v = mu, col = &quot;red&quot;) min_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))] abline(h = min_sd, col = &quot;blue&quot;) plot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = &#39;l&#39;, main = paste0(&quot;CRPS profile given mu = &quot;, mu)) abline(v = min_sd, col = &quot;blue&quot;) Next, we plot the relationship between a given value of \\(\\mu\\) and the \\(\\sigma\\) that produces the optimal CRPS. This looks like a linear relationship. optimal_sd &lt;- rep(0, length(sample_mean)) for (i in 1:length(sample_mean)) { optimal_sd[i] &lt;- sample_sd[which.min(crps_norm(y, mean = sample_mean[i], sd = sample_sd))] } plot(sample_mean, optimal_sd, type = &#39;l&#39;) Let’s estimate the slope of the relationship. It looks like the optimal \\(sd\\) for a normal distribution forecast that is biased by \\(|y - \\mu|\\) is \\(sd = 1.2|y - \\mu|\\) which makes sense as this would put the true value in a region of high probability. coef(lm(optimal_sd[sample_mean &gt; 0] ~ sample_mean[sample_mean &gt; 0])) ## (Intercept) sample_mean[sample_mean &gt; 0] ## 2.430864e+00 -1.688326e-16 8.3 Null forecast All forecasts will be compared to a null forecast produced by a simple historical-means calculation or a random walk. The GitHub repository for each theme has the code for the null model. 8.4 Forecast Submission Visualization and Leaderboard The dashboard shows the forecast submissions by each team for each forecast theme by date and forecast variable. It also provides the CRPS scores for each submitted forecast. "],["helpful-functions.html", "9 Helpful functions", " 9 Helpful functions Adding examples of helpful functions "],["frequently-asked-questions.html", "10 Frequently Asked Questions", " 10 Frequently Asked Questions "]]
